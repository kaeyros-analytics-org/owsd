[{"path":"index.html","id":"welcome-adress","chapter":"1 Welcome adress","heading":"1 Welcome adress","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed eiusmod tempor incididunt ut labore et dolore magna aliqua. Interdum velit laoreet id donec. Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida. Condimentum lacinia quis vel eros donec ac. Egestas erat imperdiet sed euismod nisi porta lorem. Montes nascetur ridiculus mus mauris vitae ultricies. Hendrerit dolor magna eget est lorem ipsum. Dictum fusce ut placerat orci nulla. Integer eget aliquet nibh praesent tristique magna sit amet purus. Aliquam purus sit amet luctus venenatis lectus magna fringilla. Pulvinar mattis nunc sed blandit libero volutpat sed cras. elementum eu facilisis sed odio morbi quis. egestas erat imperdiet sed euismod nisi porta lorem. Ac placerat vestibulum lectus mauris ultrices eros. Placerat egestas erat imperdiet. Curabitur gravida arcu ac tortor dignissim. Libero enim sed faucibus turpis eu mi bibendum. Vulputate dignissim suspendisse est ante nibh.quis risus sed vulputate odio ut enim blandit. Quis blandit turpis cursus hac habitasse platea. Et tortor consequat id porta nibh venenatis cras sed. Aliquet bibendum enim facilisis gravida. Proin nibh nisl condimentum id venenatis condimentum. Vivamus augue eget arcu dictum varius duis. Ut eu sem integer vitae justo eget magna. Tristique magna sit amet purus gravida quis blandit turpis cursus. Enim lobortis scelerisque fermentum dui faucibus ornare. Adipiscing enim eu turpis egestas pretium aenean.Diam arcu cursus euismod. Mauris augue neque gravida fermentum. Amet cursus sit amet dictum sit amet. Velit euismod pellentesque massa placerat duis ultricies. Natoque penatibus et magnis dis parturient montes nascetur ridiculus mus. Enim eu turpis egestas pretium. Vulputate enim nulla aliquet porttitor lacus luctus accumsan. Tortor id aliquet lectus proin nibh nisl. Ac felis donec et odio pellentesque diam volutpat commodo sed. Ut diam quam nulla porttitor massa id. Quam quisque id diam vel quam elementum pulvinar etiam. Fames ac turpis egestas maecenas pharetra. Vulputate odio ut enim blandit volutpat maecenas volutpat. Consequat mauris nunc congue nisi. Pellentesque adipiscing commodo elit . Tortor vitae purus faucibus ornare. Semper eget duis tellus urna. Ornare quam viverra orci sagittis eu volutpat odio. Elementum facilisis leo vel fringilla est ullamcorper eget nulla.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"introduction-to-r-and-rstudio","chapter":"2 Introduction to R and RStudio","heading":"2 Introduction to R and RStudio","text":"R widely used programming language data analysis data science. open-source free nature makes accessible everyone, active community offers invaluable support users.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"presentation","chapter":"2 Introduction to R and RStudio","heading":"2.1 Presentation","text":"R popular programming language free open source software data analysis science. particularly powerful performing complex statistical calculations creating attractive graphics. R offers around 20,000 packages compatible variety operating systems.RStudio integrated development environment (IDE) specifically designed work R programming language. makes working R easier enjoyable.RStudio’s key features numerous, just :User-friendly interface: RStudio intuitive interface code editor, console, environment panel plot panel. makes easy navigate visualise work.Code editing features: RStudio offers code editing features code completion, syntax highlighting debugging, allowing write R code faster easier.Package management: RStudio makes easy install manage many packages available R extend functionality specific tasks.Data visualization: RStudio makes easy create graphs visualizations data, helping explore trends communicate results.ability create projects organise share work colleagues effectively.History environment: RStudio keeps history orders variables, can keep track work easily re-use previous elements.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"when-was-r-created","chapter":"2 Introduction to R and RStudio","heading":"When was R created?","text":"R created early 1990s University Auckland statisticians Ross Ihaka Robert Gentleman.\nIhaka Gentleman, statistics professors New Zealand university, saw Ihaka called “common need better software environment” computer science laboratories. realization prompted pair begin developing R, implementation earlier S programming language.\nAlthough professors started working R early 90s, version 1.0.0 wasn’t officially released February 2000.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"why-the-name-r","chapter":"2 Introduction to R and RStudio","heading":"Why the name R ?","text":"R language takes name two sources: firstly, first letter name creators, secondly, play words name predecessor, S language, originally designed Bell Telephone Laboratories.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"strengths-of-r","chapter":"2 Introduction to R and RStudio","heading":"Strengths of R","text":"free software: advantage free encouraging reproducible research;free software: advantage free encouraging reproducible research;interpreted language: language closer language machine language, simpler direct , example, C C++;interpreted language: language closer language machine language, simpler direct , example, C C++;easier code sharing re-use thanks package system CRAN;easier code sharing re-use thanks package system CRAN;active community developers users:\nR evolves quickly, bugs quickly identified corrected;\nlot information programming R Internet;\nnumber R packages always growing, new features frequently added R.\nactive community developers users:R evolves quickly, bugs quickly identified corrected;lot information programming R Internet;number R packages always growing, new features frequently added R.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"weaknesses-of-r","chapter":"2 Introduction to R and RStudio","heading":"Weaknesses of r","text":"Performance Limitations: R typically slower compiled languages like C++ Java computationally intensive tasks involving large datasets. can bottleneck dealing complex models big data analysis.Basic security: R lacks basic security features, essential programming languages like Python. Consequently, limitations embedding R web applications.Complicated Language: R easy language learn steep learning curve. Individuals without prior programming experience may find challenging learn R.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"installation-of-r","chapter":"2 Introduction to R and RStudio","heading":"2.2 Installation of R","text":"install Windows, go http://cran.r-project.org/bin/windows/base/ follow first link download installer. installer launched, simply install R default options.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"installation-of-rstudio","chapter":"2 Introduction to R and RStudio","heading":"2.3 Installation of RStudio","text":"R correctly installed, go http://www.rstudio.com/products/rstudio/download/ download latest stable version RStudio. Specifically, Open Source edition RStudio Desktop (also server version).Choose installer operating system follow instructions installation program. want try latest RStudio features, can download development version (feature-rich stable version, may contain bugs) \nhttp://www.rstudio.com/products/rstudio/download/preview/.","code":""},{"path":"basics-of-the-r-language.html","id":"basics-of-the-r-language","chapter":"3 Basics of the R language","heading":"3 Basics of the R language","text":"","code":""},{"path":"basics-of-the-r-language.html","id":"variables","chapter":"3 Basics of the R language","heading":"3.1 Variables","text":"Variables identifier named space memory, stored can referenced manipulated later program.","code":""},{"path":"basics-of-the-r-language.html","id":"rule-variable-in-r","chapter":"3 Basics of the R language","heading":"Rule variable in R","text":"recommended use nouns name variable. Use underscores (e.g. donnees_menages) rather CamelCase (e.g. donneesMenages). prefer camelCase, use systematically throughout script standardise code.Notes:use T F name variables (abbreviations Booleans TRUE FALSE);use names already basic R functions(mean example). doesn’t always generate errors, prevent errors difficult detect!variable name must start letter can contain number,letter,underscore(’_‘) period(’.’).Special characters ‘#’, ‘&’, etc., along White space (tabs, space) allowed variable name.Underscore(’_’) beginning variable name allowed","code":""},{"path":"basics-of-the-r-language.html","id":"variable-assignment","chapter":"3 Basics of the R language","heading":"Variable assignment","text":"Variables R can assigned one three ways.Assignment Operator: = used assign value.following example contains 20 value stored variable ‘first.variable’ Example: first.variable = 20Assignment Operator: = used assign value.following example contains 20 value stored variable ‘first.variable’ Example: first.variable = 20<- Operator: following example contains New Program character gets assigned ‘second_variable’.\nExample: second_variable <- “New Program”<- Operator: following example contains New Program character gets assigned ‘second_variable’.\nExample: second_variable <- “New Program”-> Operator: following example contains 565 integer gets assigned ‘third.variable’.\nExample: 565 -> third.variable-> Operator: following example contains 565 integer gets assigned ‘third.variable’.\nExample: 565 -> third.variable","code":""},{"path":"basics-of-the-r-language.html","id":"types","chapter":"3 Basics of the R language","heading":"3.2 Types","text":"programming, data type important concept. Variables can store data different types, different types can different things.R, variables need declared particular type, can even change type set:several types variable R, common :integer: whole numbersnumeric: decimalscharacter: textlogical: booleans (TRUE FALSE)factor : categoriesIn addition variable types R, also data types, including:vectors: vector simply list items type.list:\nLists R objects contain elements different types like − numbers, strings, vectors another list inside . list can also contain matrix function elements. List created using list() function.matrix :\nmatrix two dimensional data structure variables typedataframe :\ndataframe two dimensional data structure variables differents types.","code":"\nval <- 3 #val is type of numeric\nval <- \"Hello\" #val is now a type of character\nclass(1L)\n#> [1] \"integer\"\nclass(1.0)\n#> [1] \"numeric\"\nclass(\"This is an R course\")\n#> [1] \"character\"\nclass(TRUE)\n#> [1] \"logical\"\nfactor.1 <- as.factor(c(\"green\",\"blue\",\"red\"))\nclass(factor.1)\n#> [1] \"factor\"\nvector_1 <- c(1,8)\nprint(vector_1)\n#> [1] 1 8\nvector_2 <- c(1,\"diamond\") #1 will become a character because all the elements \n                           #in the vector are supposed to have the same type\nprint(vector_2)\n#> [1] \"1\"       \"diamond\"\n# Create a list containing strings, numbers, vectors and a logical values.\nlist_data <- list(\"Red\", c(21,32,11), TRUE)\nprint(list_data)\n#> [[1]]\n#> [1] \"Red\"\n#> \n#> [[2]]\n#> [1] 21 32 11\n#> \n#> [[3]]\n#> [1] TRUE\nmatrix(1:9, nrow = 3, ncol = 3)\n#>      [,1] [,2] [,3]\n#> [1,]    1    4    7\n#> [2,]    2    5    8\n#> [3,]    3    6    9\ndata <- data.frame(id = c(1, 2), Age = c(21, 15), Name = c(\"John\", \"Dora\"))\nprint(data)\n#>   id Age Name\n#> 1  1  21 John\n#> 2  2  15 Dora"},{"path":"basics-of-the-r-language.html","id":"operators","chapter":"3 Basics of the R language","heading":"3.3 Operators","text":"Operators R can mainly classified following categories: arithmetic Operators,relational Operators, logical Operators,assignment OperatorsR arithmetics operators:      - addition (+)      - subtraction(-)      - multiplication (*)      - division (/)      - exponent (^)      - modulus (%%)      - integer division(%/%)Relational operators:      - less (<)      - greater (>)      - less equal (<=)      - greater equal (>=)      - equal (==)      - equal (!=)Logical operators:      - logical (!)      - Logical (&)      - Logical (|)Assignment operators:      - Leftwards assignment (<-,<<-)      - Rightwards assignment (->, ->>)","code":"\nprint(5+2)\n#> [1] 7\nprint(1-9)\n#> [1] -8\nprint(6*500)\n#> [1] 3000\nprint(5/2)\n#> [1] 2.5\nprint(2^3)\n#> [1] 8\nprint(9%%2)\n#> [1] 1\nprint(9%/%2)\n#> [1] 4\nprint(5<10)\n#> [1] TRUE\nprint(2>8)\n#> [1] FALSE\nprint(5<=5)\n#> [1] TRUE\nprint(5>=4)\n#> [1] TRUE\nx <- 7\nprint(x == 7)\n#> [1] TRUE\ny = 6\nprint(y!=4)\n#> [1] TRUE\nx <- c(TRUE, FALSE, 0, 6)\ny <- c(FALSE, TRUE, FALSE, TRUE)\n!x\n#> [1] FALSE  TRUE  TRUE FALSE\nx & y\n#> [1] FALSE FALSE FALSE  TRUE\nx | y\n#> [1]  TRUE  TRUE FALSE  TRUE\nx <- 5\nx <<- 6\n5 -> x\n6 ->>x"},{"path":"functions-and-packages.html","id":"functions-and-packages","chapter":"4 Functions and packages","heading":"4 Functions and packages","text":"embark R programming journey, understanding utilizing functions packages instrumental success. powerful tools empower tackle complex data analysis tasks, create insightful visualizations, develop innovative applications. Embrace world functions packages, unlock boundless possibilities R.","code":""},{"path":"functions-and-packages.html","id":"r-flow-control","chapter":"4 Functions and packages","heading":"4.1 R flow control","text":"run code, R executes statements order appear page, top bottom. Programming languages like R let change order code executes, allows skip certain statements run certain statements . Programming constructs let alter order code executes known control flow statements. R programming, many types control statements popular : condition, -else condition, loop, loop.condition: control structure checks expression provided parenthesis true . true, execution statements braces {} continues.\nSyntax:\n  (expression){\n    statements\n    ....\n  }condition: control structure checks expression provided parenthesis true . true, execution statements braces {} continues.\nSyntax:Example:-else condition:similar condition test expression condition fails, statements else condition executed.\nSyntax:\n  (expression) {\n    statements\n    ....\n  } else {\n    statements\n    ....\n  }-else condition:similar condition test expression condition fails, statements else condition executed.\nSyntax:Example:loop: type loop sequence statements executed repeatedly exit condition reached.\nSyntax:\n(value vector) {\nstatements\n….\n}\nExample:loop: loop another kind loop iterated condition satisfied. testing expression checked first executing body loop.\nSyntax:\n(expression) {\nstatement\n….\n}\nExample:","code":"  if(expression){\n    statements\n    ....\n  }\nx <- 100\n\nif(x > 10){ \n  print(paste(x, \"is greater than 10\"))\n}\n#> [1] \"100 is greater than 10\"  if (expression) {\n    statements\n    ....\n  } else {\n    statements\n    ....\n  }\nx <- 5\n\n# Check value is less than or greater than 10 \nif(x > 10){ \n  print(paste(x, \"is greater than 10\")) \n}else{ \n  print(paste(x, \"is less than 10\")) \n}\n#> [1] \"5 is less than 10\"\nx <- letters[3:5] \n\nfor(i in x){ \n  print(i) \n}\n#> [1] \"c\"\n#> [1] \"d\"\n#> [1] \"e\"\nx = 3\n\n# Print 1 to 5 \nwhile(x <= 5){ \n  print(x) \n  x = x + 1\n}\n#> [1] 3\n#> [1] 4\n#> [1] 5"},{"path":"functions-and-packages.html","id":"functions","chapter":"4 Functions and packages","heading":"4.2 Functions","text":"function set statements organized together perform specific task. useful want perform certain task multiple times.R function created using keyword function. basic syntax R function definition follows:Example1 : Single Input Single OutputExample 2: Multiple Input Multiple OutputExample 3: Inline FunctionExample 4: Function without Argument","code":"    function_name <- function(arg_1, arg_2,..) \n                        function body\n    }\n    \n# A simple R function to calculate \n# area of a circle\n \nareaOfCircle = function(radius){\n  area = pi*radius^2\n  return(area)\n}\n \nprint(areaOfCircle(2))\n#> [1] 12.56637\n\n# A simple R function to calculate area and perimeter of a rectangle\n \nRectangle = function(length, width){\n  area = length * width\n  perimeter = 2 * (length + width)\n   \n  # create an object called result which is a list of area and perimeter\n  result = list(\"Area\" = area, \"Perimeter\" = perimeter)\n  return(result)\n}\n \nresultList = Rectangle(2, 3)\nprint(resultList[\"Area\"])\n#> $Area\n#> [1] 6\nprint(resultList[\"Perimeter\"])\n#> $Perimeter\n#> [1] 10\n# A simple R program to demonstrate the inline function\n\nf = function(x) x^2*4+x/3\n\nprint(f(4))\n#> [1] 65.33333\nprint(f(-2))\n#> [1] 15.33333\nprint(0)\n#> [1] 0\n# Generate a random number between 0 and 1\ngenerate_random_number <- function() {\n  random_number <- runif(1)\n  return(random_number)\n}"},{"path":"functions-and-packages.html","id":"function-components","chapter":"4 Functions and packages","heading":"Function Components","text":"different parts function :Function Name: actual name function. stored R environment object name.Function Name: actual name function. stored R environment object name.Arguments: argument placeholder. function invoked, pass value argument. Arguments optional; , function may contain arguments. Also arguments can default values.Arguments: argument placeholder. function invoked, pass value argument. Arguments optional; , function may contain arguments. Also arguments can default values.Function Body: function body contains collection statements defines function .Function Body: function body contains collection statements defines function .Return Value: return value function last expression function body evaluated.Return Value: return value function last expression function body evaluated.R many -built functions can directly called program without defining first. can also create use functions referred user defined functions.","code":""},{"path":"functions-and-packages.html","id":"built-in-function","chapter":"4 Functions and packages","heading":"Built-in Function","text":"Built-Function functions already existing R language just need call use.several predefined functions, mathematical functions (abs(),sqrt(),exp(),…), statistical functions (mean(), median(), cor(),…), data manipulation functions (aggregate(),subset(),order(),…) file input/output functions (read.csv(),write.csv(),readRDS(),…).","code":""},{"path":"functions-and-packages.html","id":"packages","chapter":"4 Functions and packages","heading":"4.3 Packages","text":"Packages R Programming language set R functions, compiled code, sample data. stored directory called “library” within R environment. default, R installs group packages installation. start R console, default packages available default. packages already installed need loaded explicitly utilized R program ’s getting use .","code":""},{"path":"functions-and-packages.html","id":"repositories","chapter":"4 Functions and packages","heading":"Repositories","text":"repository place packages located stored can install R packages . Organizations Developers local repository, typically online accessible everyone. popular repositories R packages :CRAN:\nComprehensive R Archive Network(CRAN) official repository, network FTP web servers maintained R community around world. R community coordinates , package published CRAN, Package needs pass several tests ensure package following CRAN policies.\n  install.packages(\"package_name\")CRAN:\nComprehensive R Archive Network(CRAN) official repository, network FTP web servers maintained R community around world. R community coordinates , package published CRAN, Package needs pass several tests ensure package following CRAN policies.Bioconductor:\nBioconductor topic-specific repository, intended open source software bioinformatics. Similar CRAN, submission review processes, community active several conferences meetings per year order maintain quality.\ndownload repository install fist BiocManager package run:\nBiocManager::install(\"package_name\")Bioconductor:\nBioconductor topic-specific repository, intended open source software bioinformatics. Similar CRAN, submission review processes, community active several conferences meetings per year order maintain quality.\ndownload repository install fist BiocManager package run:Github:\nGithub popular repository open-source projects. ’s popular comes unlimited space open source, integration git, version control software, ease share collaborate others.\ninstall R packages GitHub first, need install devtools running following code:\n  install.packages(\"devtools\")Github:\nGithub popular repository open-source projects. ’s popular comes unlimited space open source, integration git, version control software, ease share collaborate others.\ninstall R packages GitHub first, need install devtools running following code:devtools installed, can use install_github() function install R package GitHub. syntax :can also install packages RStudio manually:\nR Studio go Tools -> Install Package, get pop-window type package want install:","code":"  install.packages(\"package_name\")BiocManager::install(\"package_name\")  install.packages(\"devtools\")    devtools::install_github(\"github_username/github_repo\")"},{"path":"functions-and-packages.html","id":"how-to-load-packages-in-r-programming-language","chapter":"4 Functions and packages","heading":"How to Load Packages in R Programming Language","text":"R package installed, ready use functionalities. just need sporadic use functions data inside package can access following notation.\ncan use library() require() load packages.load one package time:","code":"\nlibrary(stats)\nrequire(stats)\nlibrary(caret, ggplot2)"},{"path":"data-manipulation.html","id":"data-manipulation","chapter":"5 Data manipulation","heading":"5 Data manipulation","text":"Data manipulation involves modifying data make easier read organized. manipulate data analysis visualization. times, data collection process done machines involves lot errors inaccuracies reading. Data manipulation also used remove inaccuracies make data accurate precise.","code":""},{"path":"data-manipulation.html","id":"importation-of-data","chapter":"5 Data manipulation","heading":"5.1 Importation of data","text":"Data import essential step data analysis process. involves retrieving data various sources, local files, databases, APIs real-time feeds. step acquires data needed analysis decision-making, often starting point analytical work.part, learn load commonly used CSV, Excel, JSON, Database, XML/HTML data files R. Moreover, also look less commonly used file formats SPSS Stata.Importing data csv R:Importing data excel R:Importing data json R:Importing data database R:Importing data spss R:Importing data stata R:","code":"\n#load data\nbreast_cancer <- read.csv(\"./data/Breast_cancer_data.csv\")\n#load package\nlibrary(readxl)\n\n#load data\ndata_1 <- readxl::read_excel(\"./data/data_for_workshop1.xls\")\nhead(data_1)\n#> # A tibble: 6 × 42\n#>   Sex    Age       `Are you married?` Do you  have childre…¹\n#>   <chr>  <chr>     <chr>              <chr>                 \n#> 1 Male   25_40 ye… NO                 NO                    \n#> 2 Male   15_25 ye… NO                 NO                    \n#> 3 Male   25_40 ye… NO                 NO                    \n#> 4 Male   15_25 ye… NO                 NO                    \n#> 5 Female 15_25 ye… NO                 NO                    \n#> 6 Male   25_40 ye… NO                 NO                    \n#> # ℹ abbreviated name: ¹​`Do you  have children?`\n#> # ℹ 38 more variables:\n#> #   `How many children do you have?` <dbl>,\n#> #   `What is your religion?` <chr>,\n#> #   `Do have a tasba farm?` <chr>,\n#> #   `What's the size of your farm?` <chr>,\n#> #   `How long have you been growing tasba?` <chr>, …\n#load package\nlibrary(jsonlite)\n\n#load data\ndata_json <- jsonlite::fromJSON(\"./data/sample4.json\")\n\n#transform data into dataframe\nas.data.frame(data_json)\n#load package\nlibrary(RSQLite)\n\n#establish the connection to the database\nconn <- dbConnect(RSQLite::SQLite(), \"./data/mental_health.sqlite\")\n\n#list names of all the tables in the database\ndbListTables(conn)\n#> [1] \"Answer\"   \"Question\" \"Survey\"\n#retrieve data from table Question\ndata_sqlite <- dbGetQuery(conn, \"SELECT * FROM Survey\")\nhead(data_sqlite)\n#>   SurveyID                   Description\n#> 1     2014 mental health survey for 2014\n#> 2     2016 mental health survey for 2016\n#> 3     2017 mental health survey for 2017\n#> 4     2018 mental health survey for 2018\n#> 5     2019 mental health survey for 2019\n#load package\nlibrary(haven)\n\n#load data\ndata_spss <- haven::read_sav(\"./data/mental_health.sav\")\nhead(data_spss)\n#> # A tibble: 6 × 14\n#>   `CASE#` GENDER  HOME   AGE    EB    PH    CS    ER   TRO\n#>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1      42      1     0    59    NA    NA    NA    32    25\n#> 2      37      0     0    58     2     2    NA    25    19\n#> 3     141      0     0    83    NA     2     2    31    32\n#> 4     143      0     0    85     2     3    NA    18    25\n#> 5     128      1     0    75    14     8    21    20    19\n#> 6      12      1     0    50    17     9    27    28    26\n#> # ℹ 5 more variables: MOB <dbl>, SS <dbl>, CESD <dbl>,\n#> #   SelfEsteem <dbl>, Satisfaction <dbl>\n#load data\ndata_stata <- haven::read_dta(\"./data/SMOKE.DTA\")\nhead(data_stata)\n#> # A tibble: 6 × 10\n#>    educ cigpric white   age income  cigs restaurn lincome\n#>   <dbl>   <dbl> <dbl> <dbl>  <dbl> <dbl>    <dbl>   <dbl>\n#> 1  16      60.5     1    46  20000     0        0    9.90\n#> 2  16      57.9     1    40  30000     0        0   10.3 \n#> 3  12      57.7     1    58  30000     3        0   10.3 \n#> 4  13.5    57.9     1    30  20000     0        0    9.90\n#> 5  10      58.3     1    17  20000     0        0    9.90\n#> 6   6      59.3     1    86   6500     0        0    8.78\n#> # ℹ 2 more variables: agesq <dbl>, lcigpric <dbl>"},{"path":"data-manipulation.html","id":"basic-exploration-of-data","chapter":"5 Data manipulation","heading":"5.2 Basic exploration of data","text":"Data exploration helps explore think data ’re working. goal data exploration understand, visualize data can discover insights, relationships, patterns, anomalies.\nexplore data R many functions achieve .Function head(): used view first rows dataset.Function tail(): used view last rows dataset.Function str(): used provide structure data frame, showing data types.Function dim(): used know number rows columns.Function summary(): gives overview data, including minimum maximum values, quartiles, .Function table(): used build contingency table counts combination factor levels.Function unique(): unique() function R used eliminate delete duplicate values rows present vector, data frame, matrix well.Function hist(): function plot basic histogram view distribution variable.Function boxplot(): function plot boxplot, provides compact summary data’s central tendency, spread, potential outliers.","code":"\nhead(data_1,3)\ntail(data_1)\nstr(data_1)\ndim(data_1)\n#> [1] 107  42\nsummary(data_1)\ntable(data_1$Sex)\n#> \n#> Female   Male \n#>     58     49\nunique(data_1$`Do you  have children?`)\n#> [1] \"NO\"  \"YES\"\nhist(data_1$`How many children do you have?`,\n     xlab = 'Number of childrens',\n     main = 'Number of childrens')\nboxplot(data_1$`How many children do you have?`)"},{"path":"data-manipulation.html","id":"data-manipulation-with-dplyr","chapter":"5 Data manipulation","heading":"5.3 Data manipulation with dplyr","text":"IMPORTANT POINT:\nOne useful ways use dplyr pipe operator. pipe operator looks like : %>% ,common practice use pipe operator “pipe” dplyr commands together. way chain multiple operations together concise precise way. %>% operator takes output expression left passes first argument function right.order manipulate clean data, R provides library called dplyr consists many built-methods manipulate data. use data manipulation function, first need import dplyr package using library(dplyr) line code. list fundamental data manipulation verbs use data manipulations.filter():\nfilter() function used produce subset data satisfies condition specified filter() method. condition, can use conditional operators, logical operators, NA values, range operators etc. filter data. Syntax filter() function given :\n  filter(dataframeName,condition)filter():filter() function used produce subset data satisfies condition specified filter() method. condition, can use conditional operators, logical operators, NA values, range operators etc. filter data. Syntax filter() function given :Example:distinct():\ndistinct() method removes duplicate rows data frame based specified columns. syntax distinct() method given :\n  distinct(dataframeName, col1, col2,.., .keep_all=TRUE)distinct():distinct() method removes duplicate rows data frame based specified columns. syntax distinct() method given :Example:arrange():\nR, arrange() method used order rows based specified column. syntax arrange() method specified :\n  arrange(dataframeName, columnName)arrange():R, arrange() method used order rows based specified column. syntax arrange() method specified :Example:select():\nselect() method used extract required columns table specifying required column names select() method. syntax select() method mentioned :\n  select(dataframeName, col1,col2,…)select():select() method used extract required columns table specifying required column names select() method. syntax select() method mentioned :Example:rename():\nrename() function used change column names. can done syntax:\n  rename(dataframeName, newName=oldName)rename():rename() function used change column names. can done syntax:Example:mutate():\nmutate() function creates new variables without dropping old ones. syntax mutate() specified :\n  mutate(dataframeName, newVariable=formula)mutate():mutate() function creates new variables without dropping old ones. syntax mutate() specified :Example:transmute():\ntransmute() function drops old variables creates new variables. syntax:\n  transmute(dataframeName, newVariable=formula)transmute():transmute() function drops old variables creates new variables. syntax:Example:summarize():\nUsing summarize method can summarize data data frame using aggregate functions like sum(), mean(), etc. Usually function used group_by() function. syntax summarize() method specified :\n  summarize(dataframeName, aggregate_function(columnName))summarize():Using summarize method can summarize data data frame using aggregate functions like sum(), mean(), etc. Usually function used group_by() function. syntax summarize() method specified :Example:","code":"  filter(dataframeName,condition)\ndplyr::filter(data_1, Sex==\"Female\")  distinct(dataframeName, col1, col2,.., .keep_all=TRUE)\ndata_1 %>% \n  dplyr::distinct()  arrange(dataframeName, columnName)\ndata_1 %>% \n  dplyr::arrange(Sex)  select(dataframeName, col1,col2,…)\ndata_1 %>% \n  dplyr::select(Sex,`Do you  have children?`)  rename(dataframeName, newName=oldName)\ndata_1 %>%\n  dplyr::rename(Status= `Are you married?`)  mutate(dataframeName, newVariable=formula)\ndata_1 %>%\n  dplyr::mutate(sex=ifelse(Sex==\"Female\", \"F\", \"M\"))  transmute(dataframeName, newVariable=formula)\ndata_1 %>%\n  dplyr::transmute(sex=ifelse(Sex==\"Female\", \"F\", \"M\"))  summarize(dataframeName, aggregate_function(columnName))\ndata_1 %>%\n  group_by(Sex) %>%\n  summarize(mean=mean(`How many children do you have?`), count=n())"},{"path":"data-cleaning.html","id":"data-cleaning","chapter":"6 Data cleaning","heading":"6 Data cleaning","text":"domain data science, R reigns supreme tool transforming raw data actionable insights.\nData cleaning, core competency R, empowers us clean, filter, transform, aggregate data, paving way meaningful analysis. introductory paragraph delves world data manipulation data cleaning R, highlighting significance exploring key concepts involved.several methods used data cleansing, including:","code":""},{"path":"data-cleaning.html","id":"renaming-colums","chapter":"6 Data cleaning","heading":"6.1 Renaming colums","text":"data cleansing, column renaming plays crucial role organizing clarifying dataset. step involves assigning meaningful consistent names columns, facilitates interpretation subsequent use analysis.","code":"\n#load the package\nlibrary(dplyr)\n\n#rename the variable \"Are you married?\"\ndata_1 %>%\n  dplyr::rename(marital_status=`Are you married?`)"},{"path":"data-cleaning.html","id":"handling-missing-data","chapter":"6 Data cleaning","heading":"6.2 Handling Missing Data:","text":"Missing data, also known missing values, common challenge encountered data analysis. refers absence information specific variables certain observations within dataset. deal missing data two options: impute data remove data.      2.1) Imputationwe can use imputation mean, median mode.        - Imputation mean:        - Imputation median:        - Imputation mode:      2.2) Removing dataThere 2 usuals methods deleting data dealing missing data: listwise dropping variables.        - Listwise:method, data observation one missing values deleted. analysis run observations complete set data.        - Dropping variables:data missing large proportion observations, may best discard variable entirely insignificant.","code":"\n#Create a new column of number of varieties\ndata_1$number_variety <- str_sub(data_1$`How many varieties do you grow on the same plot or in the same field?`, 1, 1)\n\n#verify the type of the column\nstr(data_1$number_variety)\n\n#transform the type into number\ndata_1$number_variety <- as.integer(data_1$number_variety)\n\n#impute NA values by mean\ndata_1$number_variety[is.na(data_1$number_variety)]<-round(mean(data_1$number_variety, na.rm = TRUE))\nlibrary(stringr)\n#function to extract the number of kg in the column\ndata_1$`What is the production in kg or ton/year?` <- sapply(data_1$`What is the production in kg or ton/year?`, function(x) {\n    # Extract digits using regular expression and convert to numeric\n    str_extract(x, \"\\\\d+\") %>% as.numeric()\n})\n\n#impute the column by median\ndata_1$`What is the production in kg or ton/year?`[is.na(data_1$`What is the production in kg or ton/year?`)]<-median(data_1$`What is the production in kg or ton/year?`, na.rm = TRUE)\ndata_1$`Where do you get your seeds?`[is.na(data_1$`Where do you get your seeds?`)] <- names(which.max(table(data_1$`Where do you get your seeds?`)))\nna.omit(data_1)\nsubset( data_1, select = -c(`How do you call these varieties you have?`))"},{"path":"data-cleaning.html","id":"handling-outliers","chapter":"6 Data cleaning","heading":"6.3 Handling outliers","text":"Data points far dataset’s points considered outliers. presence outliers can pose significant problems statistical analysis machine learning. can bias model parameter estimates, lead erroneous conclusions affect algorithm performance.identify outliers can handle either impute outliers value (mean, median, mode) use method capping (missing values lie outside **1.5 * IQR** limits, cap replacing observations outside lower limit value 5th percentile lie upper limit, value 95th percentile)","code":"\noutlier_values <- boxplot.stats(data_1$`How many children do you have?`)$out  # outlier values.\nboxplot(data_1$`How many children do you have?`, main=\"Number of children\", boxwex=0.1)\nmtext(paste(\"Outliers: \", paste(outlier_values, collapse=\", \")), cex=0.6)"},{"path":"data-cleaning.html","id":"removing-duplicates","chapter":"6 Data cleaning","heading":"6.4 Removing duplicates:","text":"Removing duplicates ensures data point represented , leading accurate consistent data analysis.","code":"\ndata_1 <- data_1 %>% \n  dplyr::distinct()"},{"path":"data-cleaning.html","id":"checking-data-structure","chapter":"6 Data cleaning","heading":"6.5 Checking data structure:","text":"Checking data types crucial step data analysis ensures ’re working data way ’s intended order avoid errors later analysis.can change type data across many functions like: .numeric(), .character(), .factor() etc….data right type.","code":"\nstr(data_1)"},{"path":"data-cleaning.html","id":"handling-inconsistent-categorical-data","chapter":"6 Data cleaning","heading":"6.6 Handling Inconsistent Categorical Data","text":"Categorical variables may inconsistent spellings categories. recode() function manual recoding can help standardize categories.","code":"\ndata_1 <- data_1 %>%\n  dplyr::mutate(`How do you store your seed?` = dplyr::recode(`How do you store your seed?`, \"In bags\" = \"in bags\"))"},{"path":"data-cleaning.html","id":"combine-dataframes","chapter":"6 Data cleaning","heading":"6.7 Combine dataframes","text":"Suppose dataset combines data different sources, can combine differents datasets one. combining data multiple sources, ensure data fields align correctly.\n    - Combine column    - combine row","code":"\nculture1 <- data.frame(\n  Culture = c(\"wheat\", \"maize\", \"rice\"),\n  Area = c(100, 150, 120)\n)\n\nculture2 <- data.frame(\n  Culture = c(\"wheat\", \"maize\", \"rice\"),\n  Return = c(50, 60, 45)\n)\n\nculture_final1 <- cbind(culture1, culture2)\nculture3 <- data.frame(\n  Culture = c(\"wheat\", \"maize\", \"rice\"),\n  Area = c(100, 150, 120)\n)\n\nculture4 <- data.frame(\n  Culture = c(\"potato\", \"cassava\"),\n  Area = c(250, 400)\n)\n\nculture_final2 <- rbind(culture3, culture4)"},{"path":"data-cleaning.html","id":"data-validation","chapter":"6 Data cleaning","heading":"6.8 Data Validation","text":"Data validation involves checking data predefined rules criteria. ensures data adheres specific requirements constraints.Validation checks can prevent incorrect inconsistent data entering analysis.","code":""},{"path":"data-cleaning.html","id":"regular-expressions","chapter":"6 Data cleaning","heading":"6.9 Regular expressions","text":"Regular expressions (regex) powerful tools pattern matching replacement text data. gsub() function commonly used global pattern substitution.","code":"\ndata_1$`How much does 1kg of tasba seed cost?` <- gsub(\"FCFA\", \"\", \n                        data_1$`How much does 1kg of tasba seed cost?`)knitr::knit_child(\"05-data_cleaning.Rmd\")\n#> \n#> \n#> processing file: ./05-data_cleaning.Rmd\n#> \n  |                                     \n  |                               |   0%\n  |                                     \n  |.                              |   4% [unnamed-chunk-13]\n  |                                     \n  |..                             |   7%                   \n  |                                     \n  |...                            |  11% [unnamed-chunk-14]\n  |                                     \n  |....                           |  14%                   \n  |                                     \n  |......                         |  18% [unnamed-chunk-15]\n  |                                     \n  |.......                        |  21%                   \n  |                                     \n  |........                       |  25% [unnamed-chunk-16]\n  |                                     \n  |.........                      |  29%                   \n  |                                     \n  |..........                     |  32% [unnamed-chunk-17]\n  |                                     \n  |...........                    |  36%                   \n  |                                     \n  |............                   |  39% [unnamed-chunk-18]\n  |                                     \n  |.............                  |  43%                   \n  |                                     \n  |..............                 |  46% [unnamed-chunk-19]\n  |                                     \n  |................               |  50%                   \n  |                                     \n  |.................              |  54% [unnamed-chunk-20]\n  |                                     \n  |..................             |  57%                   \n  |                                     \n  |...................            |  61% [unnamed-chunk-21]\n  |                                     \n  |....................           |  64%                   \n  |                                     \n  |.....................          |  68% [unnamed-chunk-22]\n  |                                     \n  |......................         |  71%                   \n  |                                     \n  |.......................        |  75% [unnamed-chunk-23]\n  |                                     \n  |........................       |  79%                   \n  |                                     \n  |.........................      |  82% [unnamed-chunk-24]\n  |                                     \n  |...........................    |  86%                   \n  |                                     \n  |............................   |  89% [unnamed-chunk-25]\n  |                                     \n  |.............................  |  93%                   \n  |                                     \n  |.............................. |  96% [unnamed-chunk-26]\n  |                                     \n  |...............................| 100%                   #> [1] \"\\n\\n\\n# Data cleaning\\n\\nIn the domain of data science, R reigns supreme as a tool for transforming raw data into actionable insights. \\nData cleaning, a core competency of R, empowers us to clean, filter, transform, and aggregate data, paving the way for meaningful analysis. This introductory paragraph delves into the world of data manipulation and data cleaning in R, highlighting its significance and exploring the key concepts involved.\\n\\nThere are several methods used for data cleansing, including:\\n\\n## Renaming colums\\n\\nDuring data cleansing, column renaming plays a crucial role in organizing and clarifying the dataset. This step involves assigning meaningful and consistent names to columns, which facilitates their interpretation and subsequent use in analysis.\\n\\n\\n```r\\n#load the package\\nlibrary(dplyr)\\n\\n#rename the variable \\\"Are you married?\\\"\\ndata_1 %>%\\n  dplyr::rename(marital_status=`Are you married?`)\\n```\\n\\n\\n## Handling Missing Data: \\n\\nMissing data, also known as missing values, is a common challenge encountered in data analysis. It refers to the absence of information for specific variables in certain observations within your dataset. To deal with missing data we have two options: impute data or remove data.\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1) Imputation\\n\\nwe can use imputation by mean, median or mode.\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- *Imputation by mean*:\\n\\n\\n```r\\n#Create a new column of number of varieties\\ndata_1$number_variety <- str_sub(data_1$`How many varieties do you grow on the same plot or in the same field?`, 1, 1)\\n\\n#verify the type of the column\\nstr(data_1$number_variety)\\n\\n#transform the type into number\\ndata_1$number_variety <- as.integer(data_1$number_variety)\\n\\n#impute NA values by mean\\ndata_1$number_variety[is.na(data_1$number_variety)]<-round(mean(data_1$number_variety, na.rm = TRUE))\\n```\\n\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- *Imputation by median*:\\n\\n\\n```r\\nlibrary(stringr)\\n#function to extract the number of kg in the column\\ndata_1$`What is the production in kg or ton/year?` <- sapply(data_1$`What is the production in kg or ton/year?`, function(x) {\\n    # Extract digits using regular expression and convert to numeric\\n    str_extract(x, \\\"\\\\\\\\d+\\\") %>% as.numeric()\\n})\\n\\n#impute the column by median\\ndata_1$`What is the production in kg or ton/year?`[is.na(data_1$`What is the production in kg or ton/year?`)]<-median(data_1$`What is the production in kg or ton/year?`, na.rm = TRUE)\\n\\n```\\n\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- *Imputation by mode*:\\n\\n\\n```r\\ndata_1$`Where do you get your seeds?`[is.na(data_1$`Where do you get your seeds?`)] <- names(which.max(table(data_1$`Where do you get your seeds?`)))\\n```\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2) Removing data\\n\\nThere are 2 usuals methods for deleting data when dealing with missing data: listwise and dropping variables.\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- *Listwise*:\\n\\nIn this method, all data for an observation that has one or more missing values are deleted. The analysis is run only on observations that have a complete set of data. \\n\\n\\n```r\\nna.omit(data_1)\\n```\\n\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- *Dropping variables*:\\n\\nIf data is missing for a large proportion of the observations, it may be best to discard the variable entirely if it is insignificant.\\n\\n\\n```r\\nsubset( data_1, select = -c(`How do you call these varieties you have?`))\\n```\\n\\n## Handling outliers\\n\\nData points far from the dataset’s other points are considered outliers. The presence of outliers can pose significant problems in statistical analysis and machine learning. They can bias model parameter estimates, lead to erroneous conclusions and affect algorithm performance.\\n\\n\\n```r\\noutlier_values <- boxplot.stats(data_1$`How many children do you have?`)$out  # outlier values.\\nboxplot(data_1$`How many children do you have?`, main=\\\"Number of children\\\", boxwex=0.1)\\nmtext(paste(\\\"Outliers: \\\", paste(outlier_values, collapse=\\\", \\\")), cex=0.6)\\n```\\n\\n<img src=\\\"06-descriptive_statistics_files/figure-html/unnamed-chunk-20-1.png\\\" width=\\\"672\\\" />\\n\\nAfter identify outliers you can handle it by either impute those outliers by a value (mean, median, mode) or use the method of capping (For missing values that lie outside the **1.5 * IQR** limits, we could cap it by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile)\\n\\n## Removing duplicates: \\\\\\nRemoving duplicates ensures that each data point is represented only once, leading to more accurate and consistent data for analysis.\\n\\n\\n```r\\ndata_1 <- data_1 %>% \\n  dplyr::distinct()\\n```\\n\\n## Checking data structure: \\n\\nChecking data types is a crucial step in data analysis because it ensures you're working with the data in the way it's intended in order to avoid errors later in the analysis.\\n\\n\\n```r\\nstr(data_1)\\n```\\n\\nYou can change the type of your data across many functions like: as.numeric(), as.character(), as.factor() etc....if the data is not in the right type.\\n\\n## Handling Inconsistent Categorical Data\\n\\nCategorical variables may have inconsistent spellings or categories. The recode() function or manual recoding can help standardize categories.\\n\\n\\n```r\\ndata_1 <- data_1 %>%\\n  dplyr::mutate(`How do you store your seed?` = dplyr::recode(`How do you store your seed?`, \\\"In bags\\\" = \\\"in bags\\\"))\\n```\\n\\n\\n## Combine dataframes\\n\\nSuppose the dataset combines data from different sources, we can combine differents datasets into one. When combining data from multiple sources, ensure that all data fields align correctly.\\n&nbsp;&nbsp;&nbsp;&nbsp;- Combine by column\\n\\n\\n```r\\nculture1 <- data.frame(\\n  Culture = c(\\\"wheat\\\", \\\"maize\\\", \\\"rice\\\"),\\n  Area = c(100, 150, 120)\\n)\\n\\nculture2 <- data.frame(\\n  Culture = c(\\\"wheat\\\", \\\"maize\\\", \\\"rice\\\"),\\n  Return = c(50, 60, 45)\\n)\\n\\nculture_final1 <- cbind(culture1, culture2)\\n```\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;- combine by row\\n\\n\\n```r\\nculture3 <- data.frame(\\n  Culture = c(\\\"wheat\\\", \\\"maize\\\", \\\"rice\\\"),\\n  Area = c(100, 150, 120)\\n)\\n\\nculture4 <- data.frame(\\n  Culture = c(\\\"potato\\\", \\\"cassava\\\"),\\n  Area = c(250, 400)\\n)\\n\\nculture_final2 <- rbind(culture3, culture4)\\n```\\n\\n## Data Validation\\n\\nData validation involves checking data against predefined rules or criteria. It ensures that data adheres to specific requirements or constraints.\\n\\nValidation checks can prevent incorrect or inconsistent data from entering your analysis.\\n\\n## Regular expressions\\n\\nRegular expressions (regex) are powerful tools for pattern matching and replacement in text data. The gsub() function is commonly used for global pattern substitution.\\n\\n\\n```r\\ndata_1$`How much does 1kg of tasba seed cost?` <- gsub(\\\"FCFA\\\", \\\"\\\", \\n                        data_1$`How much does 1kg of tasba seed cost?`)\\n```\\n\\n\\n\"\nsource(\"./dependencies.R\")"},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"7 Descriptive statistics","heading":"7 Descriptive statistics","text":"","code":""},{"path":"descriptive-statistics.html","id":"central-tendency-indicators","chapter":"7 Descriptive statistics","heading":"7.1 Central Tendency Indicators","text":"Central tendency indicators, also known measures central tendency, statistical measures used summarize set data finding single value represents middle center data. basically give idea data points tend cluster around.three main types central tendency indicators:Mean\ncommon one, also called average. ’s calculated adding values data set dividing number values.Median\nmiddle number arrange data set order, least greatest. even number data points, median average two middle numbers.Mode\nfrequent value data set. can multiple modes, way, couple values tie frequent.","code":"\nmean(data_1$`What is the production in kg or ton/year?`)\n#> [1] 138.3084\nmedian(data_1$`What is the production in kg or ton/year?`)\n#> [1] 8\nnames(which.max(table(data_1$`Are you married?`)))\n#> [1] \"YES\""},{"path":"descriptive-statistics.html","id":"variability-indicators","chapter":"7 Descriptive statistics","heading":"7.2 Variability indicators","text":"Variability indicators, contrast central tendency, tell spread data . describe much data points differ central value (mean, median, mode). common ways measure variability:Variance\naverage squared deviations data point mean. tells much data varies average, since uses squared values, can sensitive extreme values.Standard deviation\nsquare root variance. Standard deviation expressed units original data (e.g., meters, dollars), can easier interpret variance. also reflects much data deviates mean average.Range\nsimplest method. ’s just difference highest lowest values data set. easy calculate, range can misleading data outliers.Interquartile range (IQR)\nfocuses middle half data. represents range first quartile (Q1) third quartile (Q3). Half data falls within IQR, giving better idea spread bulk data .","code":"\nvar(data_1$`What is the production in kg or ton/year?`)\n#> [1] 296798.9\n#1st approch using the native function\nsd(data_1$`What is the production in kg or ton/year?`)\n#> [1] 544.7925\n\n#2nd approch\nsqrt(var(data_1$`What is the production in kg or ton/year?`))\n#> [1] 544.7925\nmax(data_1$`How many children do you have?`) - min(data_1$`How many children do you have?`)\n#> [1] 12\nIQR(data_1$`How many children do you have?`)\n#> [1] 4"},{"path":"descriptive-statistics.html","id":"quantiles","chapter":"7 Descriptive statistics","heading":"7.3 Quantiles","text":"Quantiles values split sorted data probability distribution equal parts. general terms, q-quantile divides sorted data q parts. commonly used quantiles special names:      Quartiles:      Deciles:      Percentiles:","code":"\nquantile(data_1$`How many children do you have?`)\n#>   0%  25%  50%  75% 100% \n#>    0    0    2    4   12\nquantile(data_1$`How many children do you have?`,probs = seq(0, 1, by = 0.1))\n#>   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#>    0    0    0    0    0    2    2    3    5    6   12\nquantile(data_1$`How many children do you have?`,probs = seq(0, 1, by = 0.01))\n#>    0%    1%    2%    3%    4%    5%    6%    7%    8%    9% \n#>  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n#>   10%   11%   12%   13%   14%   15%   16%   17%   18%   19% \n#>  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n#>   20%   21%   22%   23%   24%   25%   26%   27%   28%   29% \n#>  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n#>   30%   31%   32%   33%   34%   35%   36%   37%   38%   39% \n#>  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n#>   40%   41%   42%   43%   44%   45%   46%   47%   48%   49% \n#>  0.00  0.46  1.00  1.00  1.00  1.00  1.00  1.00  1.88  2.00 \n#>   50%   51%   52%   53%   54%   55%   56%   57%   58%   59% \n#>  2.00  2.00  2.00  2.00  2.00  2.00  2.00  2.00  2.00  2.00 \n#>   60%   61%   62%   63%   64%   65%   66%   67%   68%   69% \n#>  2.00  2.00  2.00  2.00  2.00  2.90  3.00  3.00  3.00  3.00 \n#>   70%   71%   72%   73%   74%   75%   76%   77%   78%   79% \n#>  3.00  3.00  3.00  3.38  4.00  4.00  4.00  4.00  4.68  5.00 \n#>   80%   81%   82%   83%   84%   85%   86%   87%   88%   89% \n#>  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.00  5.28  6.00 \n#>   90%   91%   92%   93%   94%   95%   96%   97%   98%   99% \n#>  6.00  6.00  6.00  6.58  7.64  8.70  9.76 10.00 10.00 11.88 \n#>  100% \n#> 12.00"},{"path":"descriptive-statistics.html","id":"contingency-table","chapter":"7 Descriptive statistics","heading":"7.4 Contingency table","text":"contingency table displays frequencies combinations two categorical variables.","code":"\ntable(data_1$Sex, data_1$`Are you married?`)\n#>         \n#>          NO YES\n#>   Female 19  39\n#>   Male   28  21"},{"path":"inferential-statistics.html","id":"inferential-statistics","chapter":"8 Inferential statistics","heading":"8 Inferential statistics","text":"Inferential statistics branch statistics aims draw conclusions population sample population. Unlike descriptive statistics, simply describes summarizes characteristics sample, inferential statistics uses statistical methods models make inferences predictions wider population sample drawn.main techniques inferential statistics include hypothesis testing, parameter estimation, analysis variance (ANOVA), regression forecasting methods. fundamental concepts associated inferential statistics:","code":""},{"path":"inferential-statistics.html","id":"population-and-sample","chapter":"8 Inferential statistics","heading":"8.1 Population and sample","text":"statistics, population entire set items draw data statistical study. can group individuals, set items, etc. makes data pool study. Generally, population refers people live particular area specific time. statistics, population refers data study interest. can group individuals, objects, events, organizations, etc. use populations draw conclusions.sample defined smaller manageable representation larger group. subset larger population contains characteristics population. sample used statistical testing population size large members observations included test.\nsample unbiased subset population best represents whole data.characteristics samples populations described numbers called statistics parameters:statistic measure describes sample (e.g., sample mean).parameter measure describes whole population (e.g., population mean).two important types estimates can make population: point estimates interval estimates.point estimate single value estimate parameter. instance, sample mean point estimate population mean.\ninterval estimate gives range values parameter expected lie. confidence interval common type interval estimate.","code":""},{"path":"inferential-statistics.html","id":"parameter-estimations","chapter":"8 Inferential statistics","heading":"8.2 Parameter estimations","text":"Parameter estimation process calculating expected value population parameter based samples taken population.","code":""},{"path":"inferential-statistics.html","id":"confidence-interval","chapter":"8 Inferential statistics","heading":"8.3 Confidence interval","text":"confidence interval uses variability around statistic come interval estimate parameter. Confidence intervals useful estimating parameters take sampling error account.\nconfidence interval associated confidence level. confidence level tells probability (percentage) interval containing parameter estimate repeat study .\n95% confidence interval means repeat study new sample exactly way 100 times, can expect estimate lie within specified range values 95 times.","code":""},{"path":"inferential-statistics.html","id":"hypothesis","chapter":"8 Inferential statistics","heading":"8.4 Hypothesis","text":"Hypothesis testing fundamental concept inferential statistics involves making decisions drawing conclusions populations based sample data. hypothesis testing, start two competing hypotheses: null hypothesis (H0) alternative hypothesis (H1). hypotheses statements population parameter(s) interest.Null Hypothesis (H0):\nnull hypothesis represents status quo default assumption. suggests significant difference effect, relationship variables population.\nnull hypothesis typically states population parameter(s) equals specific value follows specific distribution.\ndenoted H0.\nnull hypothesis represents status quo default assumption. suggests significant difference effect, relationship variables population.null hypothesis typically states population parameter(s) equals specific value follows specific distribution.denoted H0.Alternative Hypothesis (H1):\nalternative hypothesis contradicts null hypothesis suggests significant difference, effect, relationship population.\nalternative hypothesis can take different forms depending research question nature hypothesis tested.\ndenoted H1.\nalternative hypothesis contradicts null hypothesis suggests significant difference, effect, relationship population.alternative hypothesis can take different forms depending research question nature hypothesis tested.denoted H1.process hypothesis testing involves following steps:Formulate Hypotheses: Clearly state null alternative hypotheses based research; question problem.Select Significance Level: Choose significance level (α), typically set 0.05 0.01, represents probability rejecting null hypothesis actually true.Collect Data Calculate Test Statistic:Collect sample data compute test statistic measures strength evidence null hypothesis.Determine Critical Region: Determine critical region rejection region, consists values test statistic lead rejection null hypothesis.Make Decision: Compare calculated test statistic critical value(s) use p-values decide whether reject null hypothesis. test statistic falls critical region p-value less significance level, reject null hypothesis favor alternative hypothesis. Otherwise, fail reject null hypothesis.Interpret Results: Interpret results hypothesis test context research question. Draw conclusions population based sample data outcome hypothesis test.","code":""},{"path":"inferential-statistics.html","id":"statistical-tests","chapter":"8 Inferential statistics","heading":"8.5 Statistical tests","text":"various types statistical tests, designed address different research questions hypotheses. commonly used statistical tests include:T test Student testThe Student’s t-test statistical hypothesis test used determine significant difference means two groups.p-value <0.05 accept alternative hypothesis: significant difference average number ears cont treatment rh treatment groupsANOVAANOVA (Analysis variance) statistical method used analyze differences means two groups treatments.strong evidence suggests “Treatments” factor statistically significant effect “. ears”. less 0.01% chance observing result chance, means differences observed “. ears” probably due different treatments applied.Chi-Square testThe chi-square test statistical test used determine significant association two categorical variables.P-\nvalue >0.05 can conclude risk heart attack statistically significant men women.","code":"\ncorn_data <- read_excel(\"data/data_for_workshop2.xls\", sheet = \"corn_data\")\n\n# convert qualitative variables in factors\ncorn_data <- corn_data %>%\n  mutate_if(is.character, as.factor)\n\n# subset for the treatments \"cont\" and \"rh\"\ncont_data <- subset(corn_data, Treatments == \"Cont\")\nrh_data <- subset(corn_data, Treatments == \"RH\")\n\n# Test de Student pour comparer les moyennes du nombre d'épis entre les traitements \"Cont\" et \"RH\"\nt_test_result <- t_test_result <- t.test(cont_data$`Nb of ears`, rh_data$`Nb of ears`, alternative = \"two.sided\", mu = 0, conf.level = 0.95)\n\n# Affichage des résultats du test de Student\nprint(t_test_result)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  cont_data$`Nb of ears` and rh_data$`Nb of ears`\n#> t = -3.0566, df = 54.847, p-value = 0.003455\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -36.424992  -7.575008\n#> sample estimates:\n#> mean of x mean of y \n#>  14.73333  36.73333\n#\ndata_anova <- subset(corn_data, (Condition==\"rotation\" | Condition==\"non-rotation\"))\n\nanova_test <- aov(data_anova$`Nb of ears`~data_anova$Condition + data_anova$Treatments)\nsummary(anova_test)\n#>                        Df Sum Sq Mean Sq F value   Pr(>F)\n#> data_anova$Condition    1     14      14   0.030    0.863\n#> data_anova$Treatments   3  12609    4203   8.719 2.93e-05\n#> Residuals             115  55438     482                 \n#>                          \n#> data_anova$Condition     \n#> data_anova$Treatments ***\n#> Residuals                \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#load the data\nheart_attack1 <- read.csv(\"data/heart_attack_prediction_dataset.csv\")\n\n#transform the variable into factors\nheart_attack1$Heart.Attack.Risk <- factor(heart_attack1$Heart.Attack.Risk, \n                  levels = c(0, 1),\n                  labels = c(\"No\", \"Yes\"))\n\n#construct the contingency table for the test between heart attack risk ans sex\nchi_data = table(heart_attack1$Heart.Attack.Risk,heart_attack1$Sex) \nchi_data\n#>      \n#>       Female Male\n#>   No    1708 3916\n#>   Yes    944 2195\n                 \n#test of chi2\nprint(chisq.test(chi_data))\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  chi_data\n#> X-squared = 0.070494, df = 1, p-value = 0.7906"},{"path":"visualization.html","id":"visualization","chapter":"9 Visualization","heading":"9 Visualization","text":"","code":""},{"path":"visualization.html","id":"bar-chart","chapter":"9 Visualization","heading":"9.1 Bar chart","text":"bar chart representation numerical data pictorial form rectangles (bars) uniform width varying heights.” also known bar graphs.","code":"\n#construction of the dataframe\ndata_barchart <- as.data.frame(table(data_1$`What is your religion?`))\ndata_barchart <- data_barchart %>%\n    dplyr::mutate(percentage = round(100*(Freq/sum(Freq)),2),\n                  pct1 = paste0(percentage, \"%\")) %>%\n  rename(Religion=Var1)\n\n\n#plot the bar chart\nplotly::plot_ly(data_barchart, x = ~Religion,\n                  type = \"bar\",\n                  y = ~percentage,\n                  marker = list(color = \"#318CE7\"),\n                  text = paste(data_barchart$pct1, sep = \"\"), textposition = 'outside') %>%\n    layout(title = \"Number of persons by religion\"\n    )"},{"path":"visualization.html","id":"pie-chart","chapter":"9 Visualization","heading":"9.2 Pie chart","text":"pie chart type graph representing data circular form, slice circle representing fraction proportionate part whole.","code":"\n#construction of the dataframe\ndata_piechart <- as.data.frame(table(data_1$Sex))\ndata_piechart <- data_piechart %>%\n    dplyr::mutate(percentage = round(100*(Freq/sum(Freq)),2),\n                  pct1 = paste0(percentage, \"%\"))\n\n#plot the pie chart\nplotly::plot_ly(data_piechart, labels= ~Var1,\n          values= ~Freq, type=\"pie\",\n          hoverinfo = 'text',\n          textinfo = 'label+percent',\n          insidetextfont = list(color = '#FFFFFF'),\n          text = ~paste(\"Sex :\",Var1,\n                        \"<br>Number of persons :\", Freq,\n                        \"<br>Percentage :\", pct1),\n          marker = list(colors = c(\"#318CE7\", \"#89CFF0\"),\n                        line = list(color = '#FFFFFF', width = 1),showlegend = FALSE)) %>%\n    layout(title=\"\",\n           xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),\n           yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))"},{"path":"visualization.html","id":"histogram","chapter":"9 Visualization","heading":"9.3 Histogram","text":"histogram chart plots distribution numeric variable’s values series bars. bar typically covers range numeric values called bin class; bar’s height indicates frequency data points value within corresponding bin.","code":"\nlibrary(ggplot2)\n\n# Change colors\np<-ggplot(breast_cancer, aes(x=mean_texture)) + \n  geom_histogram(color=\"black\", fill=\"white\") +\n  labs(title=\"Histogram of mean texture\",x=\"mean texture\",\n       y = \"Count\")+\n  theme(plot.title = element_text(hjust = 0.5))\np"},{"path":"visualization.html","id":"scatter-plot","chapter":"9 Visualization","heading":"9.4 Scatter plot","text":"scatter plot (scatter chart, scatter graph) uses dots represent values two different numeric variables. position dot horizontal vertical axis indicates values individual data point. Scatter plots used observe relationships variables.","code":"\ntime_series <- readxl::read_excel(\"./data/data_for_workshop2.xls\", sheet = \"times_series\")\nggplot(time_series, aes(x = `Heigh of plant`,\n                   y = `Number of roots`)) +\ngeom_point()"},{"path":"visualization.html","id":"line-chart","chapter":"9 Visualization","heading":"9.5 Line chart","text":"line chart, also known line graph, visual representation data displays information series data points connected straight line segments. Line charts commonly used show trends changes time, making particularly useful illustrating temporal patterns relationships data. Line charts provide clear intuitive way visualize values evolve fluctuate specific period.","code":"\nggplot(time_series, aes(x = Month, y = `Heigh of plant`,color = Treatments,group = Treatments)) +\n  geom_line()"},{"path":"visualization.html","id":"map-visualization","chapter":"9 Visualization","heading":"9.6 Map visualization","text":"","code":"\n#load required packages\nlibrary(leaflet)\nlibrary(sf)\nlibrary(readr)\n\n\ncameroon_json <- sf::st_read(\"./data/cm.json\")\n#> Reading layer `cm' from data source \n#>   `C:\\Users\\LENOVO\\Desktop\\Projets\\bookdown\\data\\cm.json' \n#>   using driver `GeoJSON'\n#> Simple feature collection with 10 features and 3 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 8.505056 ymin: 1.654551 xmax: 16.20772 ymax: 13.08114\n#> Geodetic CRS:  WGS 84\n\n#load the data\npopulation <- read_csv(\"./data/positives_case_covid.csv\")\n\n# join the dataset cameroon_json to the dataframe of positive cases covid\ncameroon_json$population <- population$`Positive Cases`\n\n# map visualization\nleaflet(data = cameroon_json) %>%\n  addTiles() %>%\n  addPolygons(fillColor = ~colorQuantile(\"Set2\", population)(population),\n              stroke = FALSE,\n              fillOpacity = 0.7,\n              label = ~paste(name, \" : \", \"Positive Cases:\", population))"},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"introduction-to-machine-learning-and-machine-learning-techniques","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"10 Introduction to Machine Learning and Machine Learning techniques","text":"Machine learning (ML) field artificial intelligence (AI) enables computers learn data without explicitly programmed. Instead following rigid instructions, machine learning algorithms adapt improve performance according data exposed .\nmachine-learning capability revolutionizing many sectors, including finance, healthcare, marketing manufacturing. enables computers perform complex tasks previously considered exclusive domain humans, image recognition, natural language processing autonomous decision-making.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"machine-learning-techniques","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"10.1 Machine learning techniques","text":"Machine learning relies variety techniques, strengths weaknesses. common techniques include:","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"supervised-learning","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"Supervised learning","text":"Imagine ’re teaching friend identify flowers. show pictures labels like “rose” “daisy.” similar supervised machine learning. machine learns data already correct answers attached, like flower labels. way, can recognize new flowers later.\nSupervised machine learning often used create machine learning models used prediction classification purposes.\nVarious algorithms used supervised machine learning processes.    1) Linear regression\nLinear regression fundamental supervised machine learning algorithm used modeling relationship dependent variable (want predict) one independent variables (’re basing prediction ).\ngoal linear regression find best-fitting line minimizes differences observed values values predicted linear model. typically done minimizing sum squared differences observed predicted values.    2) Logistic regression\nLogistic regression another supervised machine learning algorithm, unlike linear regression, ’s specifically designed classification tasks dependent variable can limited number categories (usually two).\nLogistic regression doesn’t directly output classification. Instead, calculates probability observation belonging specific category.    3) Support vector machines\nsupport vector machine (SVM) supervised machine learning algorithm classifies data finding optimal line hyperplane maximizes distance class N-dimensional space.\nSVMs capable performing linear nonlinear classification tasks. cases classes linearly separable, SVMs can map input features higher-dimensional space using technique called kernel trick.    4) Decison trees\nDecision trees popular intuitive supervised learning algorithm used classification regression tasks machine learning. powerful tool predictive modeling widely used various domains due simplicity, interpretability, flexibility.\ndecision tree hierarchical structure consisting nodes branches. internal node represents decision based value certain feature, branch represents possible outcomes decision. leaf nodes tree represent final predictions decisions.\nprocess constructing decision tree involves recursively splitting dataset subsets based values different features, resulting subsets pure possible respect target variable.    5) Random forest\nRandom Forest algorithm powerful tree learning technique Machine Learning. works creating number Decision Trees training phase. tree constructed using random subset data set measure random subset features partition. randomness introduces variability among individual trees, reducing risk overfitting improving overall prediction performance. prediction, algorithm aggregates results trees, either voting (classification tasks) averaging (regression tasks) collaborative decision-making process, supported multiple trees insights, provides example stable precise results.    6) K-Nearest Neighbors (KNN)\nK-Nearest Neighbors (KNN) algorithm popular machine learning technique used classification regression tasks. relies idea similar data points tend similar labels values.\nKNN classifies new data points based similarity existing data points training set. identifies k nearest neighbors (data points) training data new data point predicts class label (classification) average value (regression) based majority vote (classification) average value (regression) neighbors.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"semi-supervised-learning","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"Semi-supervised learning","text":"Semi-supervised learning uses unlabeled labeled data sets train algorithms. Typically, machine semi-learning, algorithms first fed small amount labeled data guide development, much larger amounts unlabeled data complete model.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"unsupervised-learning","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"Unsupervised learning","text":"unsupervised learning, algorithms learn unlabeled data set, .e. data associated pre-existing labels categories. aim algorithm discover hidden structures patterns data.\nUnsupervised machine learning often used researchers data scientists identify patterns within large, unlabeled data sets quickly efficiently.\nunsupervised learning algorithms :    1) K-means:\nK-means popular unsupervised machine learning algorithm used partitioning dataset predefined number groups (clusters). ’s breakdown works:First step: Initialization\nspecify desired number clusters (k).algorithm randomly selects k data points initial centroids, represent center cluster.First step: Initialization\nspecify desired number clusters (k).algorithm randomly selects k data points initial centroids, represent center cluster.Second step: Assignment\ndata point dataset assigned closest centroid based distance metric (usually Euclidean distance).\n-Third step: Re-computation\ndata points assigned cluster, centroids recalculated mean points within cluster.Second step: Assignment\ndata point dataset assigned closest centroid based distance metric (usually Euclidean distance).\n-Third step: Re-computation\ndata points assigned cluster, centroids recalculated mean points within cluster.Fourth step: Iteration\nSteps 2 3 repeated iteratively:\n    -Data points reassigned closest centroid based updated centroids.\n    -Centroids recalculated based newly assigned data points.Fourth step: Iteration\nSteps 2 3 repeated iteratively:\n    -Data points reassigned closest centroid based updated centroids.\n    -Centroids recalculated based newly assigned data points.Fifth step: Stopping Criterion\niteration process continues stopping criterion met. can centroids longer significantly change iterations (convergence).Fifth step: Stopping Criterion\niteration process continues stopping criterion met. can centroids longer significantly change iterations (convergence).    2) Principle Component Analysis (PCA):\nPrincipal component analysis (PCA) type dimensionality reduction algorithm used reduce redundancies compress datasets feature extraction. method uses linear transformation create new data representation, yielding set “principal components.” first principal component direction maximizes variance dataset. second principal component also finds maximum variance data, completely uncorrelated first principal component, yielding direction perpendicular, orthogonal, first component. process repeats based number dimensions, next principal component direction orthogonal prior components variance.    3) Hierarchical clustering:\nHierarchical clustering unsupervised learning technique used group similar objects clusters. creates hierarchy clusters merging splitting based similarity measures.\nClustering Hierarchical groups similar objects dendrogram. merges similar clusters iteratively, starting data point separate cluster. creates tree-like structure shows relationships clusters hierarchy.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"reinforcement-learning","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"Reinforcement learning","text":"reinforcement learning, algorithms learn interacting environment. algorithm takes actions environment receives rewards penalties according actions. aim algorithm learn maximize cumulative reward time.\nReinforcement learning often used create algorithms must effectively make sequences decisions actions achieve aims, playing game summarizing entire text.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"concepts-of-underfitting-and-overfitting","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"10.2 Concepts of underfitting and overfitting","text":"part, ’ll focus two terms machine learning: overfitting underfitting. terms define model’s ability capture relationship input output data. possible causes poor model performance.\nOverfitting happens train machine learning model much tuned training set. result, model learns training data well, can’t generate good predictions unseen data. overfitted model produces low accuracy results data points unseen training, hence, leads non-optimal decisions.\nUnderfitting occurs machine learning model well-tuned training set. resulting model capturing relationship input output well enough. Therefore, doesn’t produce accurate predictions, even training dataset. Resultingly, underfitted model generates poor results lead high-error decisions, like overfitted model.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"detecting-underfitting-and-overfitting","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"10.2.1 Detecting underfitting and overfitting","text":"    - Detecting underfitting\nUsually, detecting underfitting straightforward detecting overfitting. Even without using test set, can decide model performing poorly training set . model accuracy insufficient training data, high bias hence, underfitting.    - Detecting overfitting\nnumber epochs increases, training accuracy typically increases. However, training accuracy continues increase validation accuracy starts decrease, indication overfitting.","code":""},{"path":"introduction-to-machine-learning-and-machine-learning-techniques.html","id":"cures-for-underfitting-and-overfitting","chapter":"10 Introduction to Machine Learning and Machine Learning techniques","heading":"10.2.2 Cures for underfitting and overfitting","text":"    1) Cures underfitting\nprevent underfitting can:Use model complexObtain training dataIncrease number features    2) Cures overfittings\nprevent overfitting can:Reduce model complexityReduce number input featuresuse training examples train model generalize better.","code":""},{"path":"data-preparation.html","id":"data-preparation","chapter":"11 Data preparation","heading":"11 Data preparation","text":"Data preparation crucial step machine learning pipeline involves cleaning, transforming, pre-processing raw data make suitable training evaluation. process ensures data format machine learning algorithms can effectively learn , ultimately improving performance generalization ability models. carefully preparing data feeding machine learning algorithms, practitioners can mitigate potential issues overfitting, improve model accuracy, facilitate meaningful insights data.","code":""},{"path":"data-preparation.html","id":"data-cleaning-1","chapter":"11 Data preparation","heading":"11.1 Data cleaning","text":"Data cleaning essential pre-processing step machine learning focuses identifying rectifying errors, inconsistencies, inaccuracies raw data. process involves tasks handling missing values, removing duplicates, correcting data format inconsistencies, dealing outliers. Data cleaning ensures dataset high quality integrity, crucial building accurate reliable machine learning models. thoroughly cleaning data, practitioners can enhance quality analyses, improve model performance, foster meaningful insights data.","code":"\n#load the data\nheart_attack <- read.csv(\"./data/heart_attack_prediction_dataset.csv\")\n\n#verify missing values\nsum(is.na(heart_attack))\n#> [1] 0\n\n#remove irrelevant columns\nheart_attack <- subset(heart_attack, select = -c(Patient.ID, Country, Continent,Hemisphere))\n\n#check duplicates observations\ntable(duplicated(heart_attack))\n#> \n#> FALSE \n#>  8763"},{"path":"data-preparation.html","id":"feature-creation","chapter":"11 Data preparation","heading":"11.2 Feature creation","text":"Feature creation, also known feature engineering, critical aspect machine learning new features derived constructed existing ones enhance model performance capture complex relationships data. process involves transforming raw input data informative representation better captures underlying patterns structures. Feature creation techniques may include mathematical transformations like log transforms, creating interaction terms existing features, binning numerical features categorical ones encoding categorical variables. Effective feature creation can significantly impact predictive power machine learning models, enabling better generalize unseen data achieve higher levels accuracy robustness.","code":"\n#feature creation\nheart_attack[c('systolic_pressure', 'diastolic_pressure')] <- stringr::str_split_fixed(heart_attack$Blood.Pressure, '/', 2)\n\n#convert the two columns in numeric\nheart_attack$`systolic_pressure` <- as.numeric(heart_attack$`systolic_pressure`)\nheart_attack$`diastolic_pressure` <- as.numeric(heart_attack$`diastolic_pressure`)\n\n#Creation of the new variable\nheart_attack$Blood_pressure_difference <- heart_attack$`systolic_pressure` - heart_attack$`diastolic_pressure`\n\n#remove the olds variable\nremove_columns <- c(\"Blood.Pressure\", \"systolic_pressure\",\"diastolic_pressure\")\nheart_attack <- heart_attack[, which(names(heart_attack) %in% remove_columns)]"},{"path":"data-preparation.html","id":"feature-scaling","chapter":"11 Data preparation","heading":"11.3 Feature scaling","text":"Feature scaling, technique used transform numerical features dataset common scale. goal bring features similar magnitude, making comparable preventing particular feature dominating learning algorithm due larger scale. Feature scaling essential preprocessing step machine learning.\ncommon methods feature scaling :Standardization: method transforms data zero mean unit variance. subtracts mean divides standard deviation feature. Standardization preserves shape original distribution useful data normal distribution.Standardization: method transforms data zero mean unit variance. subtracts mean divides standard deviation feature. Standardization preserves shape original distribution useful data normal distribution.Normalization: Normalization scales data fixed range, typically 0 1. achieved subtracting minimum value dividing range (maximum value minus minimum value) feature. Normalization suitable data bounded range follows uniform distribution.Normalization: Normalization scales data fixed range, typically 0 1. achieved subtracting minimum value dividing range (maximum value minus minimum value) feature. Normalization suitable data bounded range follows uniform distribution.","code":"\n#load the data\nbreast_cancer <- read.csv(\"data/Breast_cancer_data.csv\")\n\nbreast_cancer[,-6] <- scale(breast_cancer[,-6])"},{"path":"data-preparation.html","id":"feature-encoding","chapter":"11 Data preparation","heading":"11.4 Feature encoding","text":"Feature encoding crucial step machine learning categorical variables converted numerical representations algorithms can understand. Since many machine learning algorithms require numerical input, feature encoding transforms categorical data format preserves information contained original variables. \nCommon techniques feature encoding include one-hot encoding(creates new (binary) columns, indicating presence possible value original data), label encoding assigns unique numerical value category ordinal encoding ensure ordinal nature variables sustained.","code":"\nbreast_cancer$diagnosis <- as.factor(breast_cancer$diagnosis)"},{"path":"data-preparation.html","id":"data-reduction","chapter":"11 Data preparation","heading":"11.5 Data reduction","text":"machine learning, dimensionality reduction tackles challenge high-dimensional data. Imagine vast landscape many features representing different directions. Dimensionality reduction techniques condense data lower-dimensional space preserving important information. simplifies analysis visualization also improves performance machine learning algorithms reducing computational costs risk overfitting. Dimensionnality techniques include feature selection feature extraction.Feature selection   - Correlation analysis:\nidentifies degree linear relationship pairs features, enabling removal highly correlated features reduce redundancy multicollinearity dataset.   - Recursive Feature Elimination:\nFeature selection refers techniques select subset relevant features dataset. Fewer features can allow machine learning algorithms run efficiently (less space time complexity) effective.   - Statistical tests:\nutilizes statistical measures (e.g., t-tests, ANOVA) assess significance individual features relation target variable, facilitating selection features significantly contribute predictive power model.Feature extraction   - Principal Component Analysis (PCA):\nreduces dimensionality dataset transforming correlated features smaller set uncorrelated variables called principal components, capturing variance data.   - Linear Discriminant Analysis (LDA):\nLDA finds new features (discriminant functions) best separate different classes data. helps focus model key characteristics differentiate classes.mean_area mean_perimeter variables highly correlated, ’ll need keep one.","code":"\nlibrary(ggcorrplot)\n#Correlation matrix\np.mat <- cor_pmat(breast_cancer[,-6])\np.mat\n#>                  mean_radius mean_texture mean_perimeter\n#> mean_radius     0.000000e+00 2.360374e-15   0.000000e+00\n#> mean_texture    2.360374e-15 0.000000e+00   7.041961e-16\n#> mean_perimeter  0.000000e+00 7.041961e-16   0.000000e+00\n#> mean_area       0.000000e+00 4.124850e-15   0.000000e+00\n#> mean_smoothness 4.312577e-05 5.776966e-01   6.108608e-07\n#>                    mean_area mean_smoothness\n#> mean_radius     0.000000e+00    4.312577e-05\n#> mean_texture    4.124850e-15    5.776966e-01\n#> mean_perimeter  0.000000e+00    6.108608e-07\n#> mean_area       0.000000e+00    2.165664e-05\n#> mean_smoothness 2.165664e-05    0.000000e+00\ncorr <- cor(breast_cancer[,-6])\n\nggcorrplot(corr,\n           type = \"lower\",\n           lab = TRUE,\n           digits = 3,p.mat = p.mat)"},{"path":"data-preparation.html","id":"handling-imbalanced-dataset","chapter":"11 Data preparation","heading":"11.6 Handling imbalanced dataset","text":"Imbalanced datasets occur one class significantly outnumbers (s), leading biased model training poor generalization performance.\ncan handle imbalanced dataset applying undersampling, oversampling method SMOTE.undersampling:\nprocess undersampling counts number minority samples dataset, randomly selects number majority sample.undersampling:\nprocess undersampling counts number minority samples dataset, randomly selects number majority sample.oversampling:\nmethod repeatedly duplicates randomly selected minority classes equal number majority minority samples.oversampling:\nmethod repeatedly duplicates randomly selected minority classes equal number majority minority samples.SMOTE(Synthetic Minority Oversampling Technique):\nsimple explanation randomly selects minority data point looks nearest k minority class neighbours. randomly selects one neighbours, draws line creates new data point randomly along line. repeated minority class reached predetermined ratio majority class.SMOTE(Synthetic Minority Oversampling Technique):\nsimple explanation randomly selects minority data point looks nearest k minority class neighbours. randomly selects one neighbours, draws line creates new data point randomly along line. repeated minority class reached predetermined ratio majority class.","code":""},{"path":"model-training-and-hyperparameter-tuning.html","id":"model-training-and-hyperparameter-tuning","chapter":"12 Model training and hyperparameter tuning","heading":"12 Model training and hyperparameter tuning","text":"","code":""},{"path":"model-training-and-hyperparameter-tuning.html","id":"concept-of-train-set-test-set-validation-set-and-cross-validation","chapter":"12 Model training and hyperparameter tuning","heading":"12.1 Concept of train set, test set, validation set and cross validation","text":"training set dataset employ train model. dataset model uses learn underlying patterns relationships enable making predictions later .test set used approximate models’s true performance reality. final step evaluating model’s performance unseen data.validation set uses subset training data provide unbiased evaluation model. validation data set contrasts training test sets intermediate phase used choosing best model optimizing . phase hyperparameter tuning occurs.Cross-validation statistical method used estimate performance (accuracy) machine learning models. cross-validation, make fixed number folds (partitions) data, run modelling process fold, average overall error estimate.","code":""},{"path":"model-training-and-hyperparameter-tuning.html","id":"model-training","chapter":"12 Model training and hyperparameter tuning","heading":"12.2 Model training","text":"Model training process teaching machine learning algorithm learn patterns relationships data adjusting parameters based provided training dataset.\ntrain model use package caret.","code":"\n#import libraries\nlibrary(tidymodels)\nlibrary(caTools)\nlibrary(caret)\n\n#load the data\n#breast_cancer <- read.csv(\"data/Breast_cancer_data.csv\")\n\n#transform the target variable to factor\nbreast_cancer$diagnosis <- as.factor(breast_cancer$diagnosis)\n\n# fixing the observations in training set and test set\nset.seed(123)\n# splitting the data set into ratio 0.80:0.20\nsplit <- caTools::sample.split(breast_cancer$diagnosis, SplitRatio = 0.80)\n\n# creating training dataset\ntrainingSet <- subset(breast_cancer, split == TRUE)\n\n# creating test data set\ntestSet <- subset(breast_cancer, split == FALSE)\n\n#train the KNN model\ndefault_knn_mod = train(\n  diagnosis ~ .,\n  data = trainingSet,\n  method = \"knn\",\n  trControl = trainControl(method = \"cv\", number = 5)\n)"},{"path":"model-training-and-hyperparameter-tuning.html","id":"hyperparameter-tuning","chapter":"12 Model training and hyperparameter tuning","heading":"12.3 Hyperparameter tuning","text":"Hyperparameter tuning process selecting optimal set hyperparameters machine learning model.","code":"\n#tuning hyperparameters of the KNN model\ntune_knn_mod = train(\n  diagnosis ~ .,\n  data = trainingSet,\n  method = \"knn\",\n  trControl = trainControl(method = \"cv\", number = 5),\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = expand.grid(k = seq(1, 101, by = 2))\n)"},{"path":"model-training-and-hyperparameter-tuning.html","id":"model-evaluation","chapter":"12 Model training and hyperparameter tuning","heading":"12.4 Model evaluation","text":"Model evaluation process assessing performance effectiveness machine learning model unseen data. involves various techniques metrics measure well model generalizes new observations.","code":"\n#predictions on the test set for the first model\nmodel_pred <- predict(default_knn_mod, newdata = testSet)\n\n#confusion matrix for the fist model\ncm <- table(model_pred,testSet$diagnosis)\ncm\n#>           \n#> model_pred  0  1\n#>          0 26  1\n#>          1 16 70\n\n#predictions on the test set for the second model\nmodel_pred_tune <- predict(tune_knn_mod, newdata = testSet)\n\nconfusion_matrix <- table(model_pred_tune,testSet$diagnosis)\nconfusion_matrix\n#>                \n#> model_pred_tune  0  1\n#>               0 31  0\n#>               1 11 71\n\n#Calculate the accuracy\ncalc_acc <- function(data) {\n  data <- as.data.frame(data)\n  max_accuracy_index <- which.max(data$Accuracy)\n  row_with_max_accuracy <- data[max_accuracy_index, ]\n  return(row_with_max_accuracy$Accuracy)\n}\n\nprint(paste(\"The accuracy of the simple model is:\", calc_acc(default_knn_mod$results)))\n#> [1] \"The accuracy of the simple model is: 0.899044433827042\"\nprint(paste(\"The accuracy of the tuned model is:\", calc_acc(tune_knn_mod$results)))\n#> [1] \"The accuracy of the tuned model is: 0.936359292881032\"knitr::knit_child(\"11-modeling.Rmd\")\n#> \n#> \n#> processing file: ./11-modeling.Rmd\n#> \n  |                                      \n  |                                |   0%\n  |                                      \n  |....                            |  12% [unnamed-chunk-4]\n  |                                      \n  |........                        |  25%                  \n  |                                      \n  |............                    |  38% [unnamed-chunk-5]\n  |                                      \n  |................                |  50%                  \n  |                                      \n  |....................            |  62% [unnamed-chunk-6]\n  |                                      \n  |........................        |  75%                  \n  |                                      \n  |............................    |  88% [unnamed-chunk-7]\n  |                                      \n  |................................| 100%                  #> [1] \"\\n\\n\\n# Model training and hyperparameter tuning\\n\\n## Concept of train set, test set, validation set and cross validation\\n\\n- The training set is the dataset that we employ to train our model. It is this dataset that our model uses to learn any underlying patterns or relationships that will enable making predictions later on.\\n- The test set is used to approximate the models's true performance in the reality. It is the final step in evaluating our model's performance on unseen data.\\n- The validation set uses a subset of the training data to provide an unbiased evaluation of a model. The validation data set contrasts with training and test sets in that it is an intermediate phase used for choosing the best model and optimizing it. It is in this phase that hyperparameter tuning occurs.\\n- Cross-validation is a statistical method used to estimate the performance (or accuracy) of machine learning models. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the modelling process on each fold, and then average the overall error estimate.\\n\\n## Model training\\n\\nModel training is the process of teaching a machine learning algorithm to learn patterns and relationships in data by adjusting its parameters based on the provided training dataset.\\\\\\nTo train the model we will use the package **caret**.\\n\\n\\n```r\\n#import libraries\\nlibrary(tidymodels)\\nlibrary(caTools)\\nlibrary(caret)\\n\\n#load the data\\n#breast_cancer <- read.csv(\\\"data/Breast_cancer_data.csv\\\")\\n\\n#transform the target variable to factor\\nbreast_cancer$diagnosis <- as.factor(breast_cancer$diagnosis)\\n\\n# fixing the observations in training set and test set\\nset.seed(123)\\n# splitting the data set into ratio 0.80:0.20\\nsplit <- caTools::sample.split(breast_cancer$diagnosis, SplitRatio = 0.80)\\n\\n# creating training dataset\\ntrainingSet <- subset(breast_cancer, split == TRUE)\\n\\n# creating test data set\\ntestSet <- subset(breast_cancer, split == FALSE)\\n\\n#train the KNN model\\ndefault_knn_mod = train(\\n  diagnosis ~ .,\\n  data = trainingSet,\\n  method = \\\"knn\\\",\\n  trControl = trainControl(method = \\\"cv\\\", number = 5)\\n)\\n```\\n\\n\\n## Hyperparameter tuning\\nHyperparameter tuning is the process of selecting the optimal set of hyperparameters for a machine learning model.\\n\\n\\n```r\\n#tuning hyperparameters of the KNN model\\ntune_knn_mod = train(\\n  diagnosis ~ .,\\n  data = trainingSet,\\n  method = \\\"knn\\\",\\n  trControl = trainControl(method = \\\"cv\\\", number = 5),\\n  preProcess = c(\\\"center\\\", \\\"scale\\\"),\\n  tuneGrid = expand.grid(k = seq(1, 101, by = 2))\\n)\\n```\\n\\n\\n## Model evaluation\\\\\\nModel evaluation is the process of assessing the performance and effectiveness of a machine learning model on unseen data. It involves various techniques and metrics to measure how well the model generalizes to new observations.\\n\\n\\n```r\\n#predictions on the test set for the first model\\nmodel_pred <- predict(default_knn_mod, newdata = testSet)\\n\\n#confusion matrix for the fist model\\ncm <- table(model_pred,testSet$diagnosis)\\ncm\\n#>           \\n#> model_pred  0  1\\n#>          0 26  1\\n#>          1 16 70\\n\\n#predictions on the test set for the second model\\nmodel_pred_tune <- predict(tune_knn_mod, newdata = testSet)\\n\\nconfusion_matrix <- table(model_pred_tune,testSet$diagnosis)\\nconfusion_matrix\\n#>                \\n#> model_pred_tune  0  1\\n#>               0 31  0\\n#>               1 11 71\\n\\n#Calculate the accuracy\\ncalc_acc <- function(data) {\\n  data <- as.data.frame(data)\\n  max_accuracy_index <- which.max(data$Accuracy)\\n  row_with_max_accuracy <- data[max_accuracy_index, ]\\n  return(row_with_max_accuracy$Accuracy)\\n}\\n\\nprint(paste(\\\"The accuracy of the simple model is:\\\", calc_acc(default_knn_mod$results)))\\n#> [1] \\\"The accuracy of the simple model is: 0.899044433827042\\\"\\nprint(paste(\\\"The accuracy of the tuned model is:\\\", calc_acc(tune_knn_mod$results)))\\n#> [1] \\\"The accuracy of the tuned model is: 0.936359292881032\\\"\\n```\\n\\n\""},{"path":"model-deployment.html","id":"model-deployment","chapter":"13 Model deployment","heading":"13 Model deployment","text":"Model deployment machine learning process integrating model existing production environment can take input return output. objective enable others access utilize predictive capabilities trained machine learning model.","code":""},{"path":"model-deployment.html","id":"model-serialization","chapter":"13 Model deployment","heading":"13.1 Model serialization","text":"achieving satisfactory performance model, ’s essential serialize store future utilization. Serialization transforms model binary format suitable storage disk.Use serialized model predictions:","code":"\n#save the model\nsaveRDS(tune_knn_mod,\"./knn_model.rds\")\n#load the model\nknn_model <- readRDS(\"./knn_model.rds\")\n\n#use the model for predictions\npredict(knn_model, testSet)\n#>   [1] 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1\n#>  [28] 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n#>  [55] 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n#>  [82] 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n#> [109] 1 1 1 1 1\n#> Levels: 0 1"},{"path":"model-deployment.html","id":"model-deployment-criteria","chapter":"13 Model deployment","heading":"13.2 Model Deployment Criteria","text":"deploy model, couple criteria machine learning model needs achieve ’s ready deployment:Portability: portable model one can run efficiently various hardware configurations operating systems, minimal adjustments required.Scalability: scalable model one can handle increasing demands data volumes without compromising performance.","code":""},{"path":"model-deployment.html","id":"model-deployment-methods","chapter":"13 Model deployment","heading":"13.3 Model Deployment Methods","text":"exist three general methods deploying machine learning model:One-:\nSometimes model needed periodically. case, model can simply trained ad-hoc ’s needed pushed production deteriorates enough require fixing.Batch:\nBatch training involves updating machine learning model using subsets data time, ensuring scalability --date performance without requiring real-time predictions.Real time:\nModel deployment real-time refers process integrating trained machine learning model production environment can make predictions classifications new data instantaneously arrives.model deployed, ’s common monitor performance consider retraining updated training data. Monitoring maintaining deployment continuous delivery ensures model provides accurate reliable predictions real-world scenarios.","code":""}]
